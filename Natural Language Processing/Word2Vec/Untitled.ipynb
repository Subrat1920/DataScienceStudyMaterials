{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af3fce6-888b-4112-b363-a989056983ea",
   "metadata": {},
   "source": [
    "# Word2Vec \n",
    "Word2Vec is popular technique in Natural Langugae Processing for obtaining vector representaton of words, know as word embeddings. It was introduced by Mikolov et al. in 2013 and has since become a standard method for generating word embeddings.\n",
    "### Key Concepts\n",
    "##### 1. Word Embeddins: \n",
    "    Word2Vec represents each word in a high-dimensional vector space, where semantically similar words are mapped to nearby points. This allows words with similar meanings to have vector representation\n",
    "##### 2. Two Architectures: Word2Vec emlyos two main architecture:\n",
    "    1. Continuous Bag of Words (CBOW) : Predicts a target word based on its context (surrounding words).\n",
    "    2. Skip-Gram : Predicts the context words based on a target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66bd75-d34b-441b-9f9d-6af43a62e464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
